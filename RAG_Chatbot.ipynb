{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'source'],\n",
       "    num_rows: 2647\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=load_dataset(\"m-ric/huggingface_doc\", split=\"train\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Create an Endpoint\\n\\nAfter your first login, you will be directed to the [Endpoint creation page](https://ui.endpoints.huggingface.co/new). As an example, this guide will go through the steps to deploy [distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) for text classification. \\n\\n## 1. Enter the Hugging Face Repository ID and your desired endpoint name:\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_repository.png\" alt=\"select repository\" />\\n\\n## 2. Select your Cloud Provider and region. Initially, only AWS will be available as a Cloud Provider with the `us-east-1` and `eu-west-1` regions. We will add Azure soon, and if you need to test Endpoints with other Cloud Providers or regions, please let us know.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_region.png\" alt=\"select region\" />\\n\\n## 3. Define the [Security Level](security) for the Endpoint:\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_security.png\" alt=\"define security\" />\\n\\n## 4. Create your Endpoint by clicking **Create Endpoint**. By default, your Endpoint is created with a medium CPU (2 x 4GB vCPUs with Intel Xeon Ice Lake) The cost estimate assumes the Endpoint will be up for an entire month, and does not take autoscaling into account.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_create_cost.png\" alt=\"create endpoint\" />\\n\\n## 5. Wait for the Endpoint to build, initialize and run which can take between 1 to 5 minutes.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/overview.png\" alt=\"overview\" />\\n\\n## 6. Test your Endpoint in the overview with the Inference widget üèÅ üéâ!\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_inference.png\" alt=\"run inference\" />\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/create_endpoint.mdx'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4bc84fbefbc44418d3759e739f09d80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2647 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "database=[Document(page_content=doc['text'], metadata={'source':doc['source']}) for doc in tqdm(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content=' Create an Endpoint\n",
      "\n",
      "After your first login, you will be directed to the [Endpoint creation page](https://ui.endpoints.huggingface.co/new). As an example, this guide will go through the steps to deploy [distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) for text classification. \n",
      "\n",
      "## 1. Enter the Hugging Face Repository ID and your desired endpoint name:\n",
      "\n",
      "<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_repository.png\" alt=\"select repository\" />\n",
      "\n",
      "## 2. Select your Cloud Provider and region. Initially, only AWS will be available as a Cloud Provider with the `us-east-1` and `eu-west-1` regions. We will add Azure soon, and if you need to test Endpoints with other Cloud Providers or regions, please let us know.\n",
      "\n",
      "<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_region.png\" alt=\"select region\" />\n",
      "\n",
      "## 3. Define the [Security Level](security) for the Endpoint:\n",
      "\n",
      "<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_security.png\" alt=\"define security\" />\n",
      "\n",
      "## 4. Create your Endpoint by clicking **Create Endpoint**. By default, your Endpoint is created with a medium CPU (2 x 4GB vCPUs with Intel Xeon Ice Lake) The cost estimate assumes the Endpoint will be up for an entire month, and does not take autoscaling into account.\n",
      "\n",
      "<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_create_cost.png\" alt=\"create endpoint\" />\n",
      "\n",
      "## 5. Wait for the Endpoint to build, initialize and run which can take between 1 to 5 minutes.\n",
      "\n",
      "<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/overview.png\" alt=\"overview\" />\n",
      "\n",
      "## 6. Test your Endpoint in the overview with the Inference widget üèÅ üéâ!\n",
      "\n",
      "<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_inference.png\" alt=\"run inference\" />\n",
      "' metadata={'source': 'huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/create_endpoint.mdx'}\n",
      "2647\n"
     ]
    }
   ],
   "source": [
    "print(database[0])\n",
    "print(len(database))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunking database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_id='thenlper/gte-small'\n",
    "tokenizer=AutoTokenizer.from_pretrained(emb_id, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the markdown character separated\n",
    "md_sep= [\n",
    "    \"\\n#{1,6} \",\n",
    "    \"```\\n\",\n",
    "    \"\\n\\\\*\\\\*\\\\*+\\n\",\n",
    "    \"\\n---+\\n\",\n",
    "    \"\\n___+\\n\",\n",
    "    \"\\n\\n\",\n",
    "    \"\\n\",\n",
    "    \" \",\n",
    "    \"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chunling database\n",
    "def doc_split(data, chunk_size=512, tokenizer=tokenizer):\n",
    "    text_split=RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        tokenizer=tokenizer,\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=int(chunk_size/10),\n",
    "        add_start_index=True,\n",
    "        strip_whitespace=True,\n",
    "        separators=md_sep,\n",
    "    )\n",
    "    docs_process=[]\n",
    "    for doc in data:\n",
    "        docs_process +=text_split.split_documents([doc])\n",
    "    docs_unique=[]\n",
    "    unique_texts={}\n",
    "    for doc in docs_process:\n",
    "        if doc.page_content not in unique_texts:\n",
    "            docs_unique.append(doc)\n",
    "            unique_texts[doc.page_content]=True\n",
    "    return docs_unique, unique_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/create_endpoint.mdx', 'start_index': 1}, page_content='Create an Endpoint\\n\\nAfter your first login, you will be directed to the [Endpoint creation page](https://ui.endpoints.huggingface.co/new). As an example, this guide will go through the steps to deploy [distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) for text classification. \\n\\n## 1. Enter the Hugging Face Repository ID and your desired endpoint name:\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_repository.png\" alt=\"select repository\" />\\n\\n## 2. Select your Cloud Provider and region. Initially, only AWS will be available as a Cloud Provider with the `us-east-1` and `eu-west-1` regions. We will add Azure soon, and if you need to test Endpoints with other Cloud Providers or regions, please let us know.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_region.png\" alt=\"select region\" />\\n\\n## 3. Define the [Security Level](security) for the Endpoint:\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_security.png\" alt=\"define security\" />\\n\\n## 4. Create your Endpoint by clicking **Create Endpoint**. By default, your Endpoint is created with a medium CPU (2 x 4GB vCPUs with Intel Xeon Ice Lake) The cost estimate assumes the Endpoint will be up for an entire month, and does not take autoscaling into account.')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_process, unique_texts=doc_split(database)\n",
    "docs_process[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17995"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45445/3503239175.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  emb_model=HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "emb_model=HuggingFaceEmbeddings(\n",
    "    model_name=emb_id,\n",
    "    multi_process=True,\n",
    "    model_kwargs={'device':'cuda'},\n",
    "    encode_kwargs={'normalize_embeddings':True}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectors store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_vector=FAISS.from_documents(\n",
    "    docs_process,\n",
    "    emb_model,\n",
    "    distance_strategy=DistanceStrategy.COSINE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the database_FAISS\n",
    "database_vector.save_local('dtb_vector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading database_FAISS\n",
    "dtb_vector=FAISS.load_local('dtb_vector', emb_model, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x7f22dff34200>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtb_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id='meta-llama/Llama-3.2-1B-Instruct'\n",
    "model=AutoModelForCausalLM.from_pretrained(model_id, token=token_acces).to(device)\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_id, token=token_acces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate=pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task='text-generation',\n",
    "    do_sample=True,\n",
    "    temperature=0.01,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=500,\n",
    "    device=device\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_chat = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "\n",
    "        \"content\": \"\"\"Using the information contained in the context,\n",
    "\n",
    "give a comprehensive answer to the question.\n",
    "\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "\n",
    "Provide the number of the source document when relevant.\n",
    "\n",
    "If the answer cannot be deduced from the context, do not give an answer.\"\"\",\n",
    "\n",
    "    },\n",
    "\n",
    "    {\n",
    "\n",
    "        \"role\": \"user\",\n",
    "\n",
    "        \"content\": \"\"\"Context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Now here is the question you need to answer.\n",
    "\n",
    "Question: {question}\"\"\",\n",
    "\n",
    "    },\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template=tokenizer.apply_chat_template(\n",
    "    prompt_chat,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 18 Jan 2025\n",
      "\n",
      "Using the information contained in the context,\n",
      "\n",
      "give a comprehensive answer to the question.\n",
      "\n",
      "Respond only to the question asked, response should be concise and relevant to the question.\n",
      "\n",
      "Provide the number of the source document when relevant.\n",
      "\n",
      "If the answer cannot be deduced from the context, do not give an answer.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context:\n",
      "\n",
      "{context}\n",
      "\n",
      "---\n",
      "\n",
      "Now here is the question you need to answer.\n",
      "\n",
      "Question: {question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most relevant text to the query from the database\n",
    "user_query = \"How to create a pipeline object?\"\n",
    "query_vector = emb_model.embed_query(user_query)\n",
    "retriever_docs=dtb_vector.similarity_search(query=user_query, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "</tf>\n",
      "</frameworkcontent>\n",
      "\n",
      "## Pipeline\n",
      "\n",
      "<Youtube id=\"tiZFewofSLM\"/>\n",
      "\n",
      "The [`pipeline`] is the easiest and fastest way to use a pretrained model for inference. You can use the [`pipeline`] out-of-the-box for many tasks across different modalities, some of which are shown in the table below:\n",
      "\n",
      "<Tip>\n",
      "\n",
      "For a complete list of available tasks, check out the [pipeline API reference](./main_classes/pipelines).\n",
      "\n",
      "</Tip>\n",
      "\n",
      "\n",
      "{'source': 'huggingface/transformers/blob/main/docs/source/en/quicktour.md', 'start_index': 1585}\n"
     ]
    }
   ],
   "source": [
    "print(retriever_docs[1].page_content)\n",
    "print('\\n')\n",
    "print(retriever_docs[1].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted documents: \n",
      "Document 0:::\n",
      "```\n",
      "\n",
      "## Available Pipelines:Document 1:::\n",
      "```\n",
      "</tf>\n",
      "</frameworkcontent>\n",
      "\n",
      "## Pipeline\n",
      "\n",
      "<Youtube id=\"tiZFewofSLM\"/>\n",
      "\n",
      "The [`pipeline`] is the easiest and fastest way to use a pretrained model for inference. You can use the [`pipeline`] out-of-the-box for many tasks across different modalities, some of which are shown in the table below:\n",
      "\n",
      "<Tip>\n",
      "\n",
      "For a complete list of available tasks, check out the [pipeline API reference](./main_classes/pipelines).\n",
      "\n",
      "</Tip>Document 2:::\n",
      "```\n",
      "\n",
      "2. Pass a prompt to the pipeline to generate an image:\n",
      "\n",
      "```py\n",
      "image = pipeline(\n",
      "\t\"stained glass of darth vader, backlight, centered composition, masterpiece, photorealistic, 8k\"\n",
      ").images[0]\n",
      "imageDocument 3:::\n",
      "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n",
      "the License. You may obtain a copy of the License at\n",
      "\n",
      "http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
      "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
      "\n",
      "‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n",
      "rendered properly in your Markdown viewer.\n",
      "\n",
      "-->\n",
      "\n",
      "# How to create a custom pipeline?\n",
      "\n",
      "In this guide, we will see how to create a custom pipeline and share it on the [Hub](hf.co/models) or add it to the\n",
      "ü§ó Transformers library.\n",
      "\n",
      "First and foremost, you need to decide the raw entries the pipeline will be able to take. It can be strings, raw bytes,\n",
      "dictionaries or whatever seems to be the most likely desired input. Try to keep these inputs as pure Python as possible\n",
      "as it makes compatibility easier (even through other languages via JSON). Those will be the `inputs` of the\n",
      "pipeline (`preprocess`).\n",
      "\n",
      "Then define the `outputs`. Same policy as the `inputs`. The simpler, the better. Those will be the outputs of\n",
      "`postprocess` method.\n",
      "\n",
      "Start by inheriting the base class `Pipeline` with the 4 methods needed to implement `preprocess`,\n",
      "`_forward`, `postprocess`, and `_sanitize_parameters`.\n",
      "\n",
      "\n",
      "```python\n",
      "from transformers import PipelineDocument 4:::\n",
      "- **Self-contained**: A pipeline shall be as self-contained as possible. More specifically, this means that all functionality should be either directly defined in the pipeline file itself, should be inherited from (and only from) the [`DiffusionPipeline` class](https://github.com/huggingface/diffusers/blob/5cbed8e0d157f65d3ddc2420dfd09f2df630e978/src/diffusers/pipeline_utils.py#L56) or be directly attached to the model and scheduler components of the pipeline.\n",
      "- **Easy-to-use**: Pipelines should be extremely easy to use - one should be able to load the pipeline and\n",
      "use it for its designated task, *e.g.* text-to-image generation, in just a couple of lines of code. Most\n",
      "logic including pre-processing, an unrolled diffusion loop, and post-processing should all happen inside the `__call__` method.\n",
      "- **Easy-to-tweak**: Certain pipelines will not be able to handle all use cases and tasks that you might like them to. If you want to use a certain pipeline for a specific use case that is not yet supported, you might have to copy the pipeline file and tweak the code to your needs. We try to make the pipeline code as readable as possible so that each part ‚Äìfrom pre-processing to diffusing to post-processing‚Äì can easily be adapted. If you would like the community to benefit from your customized pipeline, we would love to see a contribution to our [community-examples](https://github.com/huggingface/diffusers/tree/main/examples/community). If you feel that an important pipeline should be part of the official pipelines but isn't, a contribution to the [official pipelines](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines) would be even better.\n"
     ]
    }
   ],
   "source": [
    "#Create the context\n",
    "retriever_docs_text=[doc.page_content for doc in retriever_docs] \n",
    "context='\\nExtracted documents: \\n'\n",
    "context+=''.join([f'Document {str(i)}:::\\n' +doc for i, doc in enumerate(retriever_docs_text)])\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 18 Jan 2025\n",
      "\n",
      "Using the information contained in the context,\n",
      "\n",
      "give a comprehensive answer to the question.\n",
      "\n",
      "Respond only to the question asked, response should be concise and relevant to the question.\n",
      "\n",
      "Provide the number of the source document when relevant.\n",
      "\n",
      "If the answer cannot be deduced from the context, do not give an answer.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context:\n",
      "\n",
      "\n",
      "Extracted documents: \n",
      "Document 0:::\n",
      "```\n",
      "\n",
      "## Available Pipelines:Document 1:::\n",
      "```\n",
      "</tf>\n",
      "</frameworkcontent>\n",
      "\n",
      "## Pipeline\n",
      "\n",
      "<Youtube id=\"tiZFewofSLM\"/>\n",
      "\n",
      "The [`pipeline`] is the easiest and fastest way to use a pretrained model for inference. You can use the [`pipeline`] out-of-the-box for many tasks across different modalities, some of which are shown in the table below:\n",
      "\n",
      "<Tip>\n",
      "\n",
      "For a complete list of available tasks, check out the [pipeline API reference](./main_classes/pipelines).\n",
      "\n",
      "</Tip>Document 2:::\n",
      "```\n",
      "\n",
      "2. Pass a prompt to the pipeline to generate an image:\n",
      "\n",
      "```py\n",
      "image = pipeline(\n",
      "\t\"stained glass of darth vader, backlight, centered composition, masterpiece, photorealistic, 8k\"\n",
      ").images[0]\n",
      "imageDocument 3:::\n",
      "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n",
      "the License. You may obtain a copy of the License at\n",
      "\n",
      "http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
      "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
      "\n",
      "‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n",
      "rendered properly in your Markdown viewer.\n",
      "\n",
      "-->\n",
      "\n",
      "# How to create a custom pipeline?\n",
      "\n",
      "In this guide, we will see how to create a custom pipeline and share it on the [Hub](hf.co/models) or add it to the\n",
      "ü§ó Transformers library.\n",
      "\n",
      "First and foremost, you need to decide the raw entries the pipeline will be able to take. It can be strings, raw bytes,\n",
      "dictionaries or whatever seems to be the most likely desired input. Try to keep these inputs as pure Python as possible\n",
      "as it makes compatibility easier (even through other languages via JSON). Those will be the `inputs` of the\n",
      "pipeline (`preprocess`).\n",
      "\n",
      "Then define the `outputs`. Same policy as the `inputs`. The simpler, the better. Those will be the outputs of\n",
      "`postprocess` method.\n",
      "\n",
      "Start by inheriting the base class `Pipeline` with the 4 methods needed to implement `preprocess`,\n",
      "`_forward`, `postprocess`, and `_sanitize_parameters`.\n",
      "\n",
      "\n",
      "```python\n",
      "from transformers import PipelineDocument 4:::\n",
      "- **Self-contained**: A pipeline shall be as self-contained as possible. More specifically, this means that all functionality should be either directly defined in the pipeline file itself, should be inherited from (and only from) the [`DiffusionPipeline` class](https://github.com/huggingface/diffusers/blob/5cbed8e0d157f65d3ddc2420dfd09f2df630e978/src/diffusers/pipeline_utils.py#L56) or be directly attached to the model and scheduler components of the pipeline.\n",
      "- **Easy-to-use**: Pipelines should be extremely easy to use - one should be able to load the pipeline and\n",
      "use it for its designated task, *e.g.* text-to-image generation, in just a couple of lines of code. Most\n",
      "logic including pre-processing, an unrolled diffusion loop, and post-processing should all happen inside the `__call__` method.\n",
      "- **Easy-to-tweak**: Certain pipelines will not be able to handle all use cases and tasks that you might like them to. If you want to use a certain pipeline for a specific use case that is not yet supported, you might have to copy the pipeline file and tweak the code to your needs. We try to make the pipeline code as readable as possible so that each part ‚Äìfrom pre-processing to diffusing to post-processing‚Äì can easily be adapted. If you would like the community to benefit from your customized pipeline, we would love to see a contribution to our [community-examples](https://github.com/huggingface/diffusers/tree/main/examples/community). If you feel that an important pipeline should be part of the official pipelines but isn't, a contribution to the [official pipelines](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines) would be even better.\n",
      "\n",
      "---\n",
      "\n",
      "Now here is the question you need to answer.\n",
      "\n",
      "Question: How to create a pipeline object<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Final prompt\n",
    "final_prompt=prompt_template.format(question='How to create a pipeline object', context=context)\n",
    "print(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'generated_text': 'To create a pipeline object, you need to inherit from the `Pipeline` class provided by the `transformers` library. Here\\'s a simple example:\\n\\n```python\\nfrom transformers import Pipeline\\n\\nclass CustomPipeline(Pipeline):\\n    def __init__(self, preprocess, postprocess, sanitize_parameters=None):\\n        super().__init__(preprocess, postprocess)\\n        self.sanitize_parameters = sanitize_parameters\\n\\n    # Define the preprocess function\\n    def preprocess(self, input_data):\\n        # Your preprocessing logic goes here\\n        return input_data\\n\\n    # Define the postprocess function\\n    def postprocess(self, output_data):\\n        # Your postprocessing logic goes here\\n        return output_data\\n\\n# Create a new pipeline object\\ncustom_pipeline = CustomPipeline(preprocess=lambda x: x, postprocess=lambda y: y)\\n\\n# Use the custom pipeline\\ninput_data = {\"text\": \"Hello World\"}\\noutput_data = custom_pipeline(input_data)\\nprint(output_data)\\n```\\n\\nThis code defines a custom pipeline class `CustomPipeline` that inherits from the `Pipeline` class. The `preprocess` and `postprocess` functions are called before and after processing the input data, respectively. In this example, the `preprocess` function simply returns the input data unchanged, while the `postprocess` function modifies the output data.'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate the answer by using the LLM model\n",
    "answer=generate(final_prompt)[0]\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To create a pipeline object, you need to inherit from the `Pipeline` class provided by the `transformers` library. Here's a simple example:\n",
      "\n",
      "```python\n",
      "from transformers import Pipeline\n",
      "\n",
      "class CustomPipeline(Pipeline):\n",
      "    def __init__(self, preprocess, postprocess, sanitize_parameters=None):\n",
      "        super().__init__(preprocess, postprocess)\n",
      "        self.sanitize_parameters = sanitize_parameters\n",
      "\n",
      "    # Define the preprocess function\n",
      "    def preprocess(self, input_data):\n",
      "        # Your preprocessing logic goes here\n",
      "        return input_data\n",
      "\n",
      "    # Define the postprocess function\n",
      "    def postprocess(self, output_data):\n",
      "        # Your postprocessing logic goes here\n",
      "        return output_data\n",
      "\n",
      "# Create a new pipeline object\n",
      "custom_pipeline = CustomPipeline(preprocess=lambda x: x, postprocess=lambda y: y)\n",
      "\n",
      "# Use the custom pipeline\n",
      "input_data = {\"text\": \"Hello World\"}\n",
      "output_data = custom_pipeline(input_data)\n",
      "print(output_data)\n",
      "```\n",
      "\n",
      "This code defines a custom pipeline class `CustomPipeline` that inherits from the `Pipeline` class. The `preprocess` and `postprocess` functions are called before and after processing the input data, respectively. In this example, the `preprocess` function simply returns the input data unchanged, while the `postprocess` function modifies the output data.\n"
     ]
    }
   ],
   "source": [
    "print(answer['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combining**\n",
    "\n",
    "We can retrieve more documents than we want, then rerank the results with a more powerful retrieval model before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_rag(question, generate=generate, data=dtb_vector, num_retrieved_docs=10, num_reranker=5, reranker=None):\n",
    "    #question_emb=emb_model.embed_query(question)\n",
    "    relevant_docs=data.similarity_search(query=question, k=num_retrieved_docs)\n",
    "    relevant_docs=[doc.page_content for doc in relevant_docs]\n",
    "    if reranker:\n",
    "        relevant_docs=reranker.rerank(question, relevant_docs, k=num_reranker)\n",
    "        relevant_docs=[doc['content'] for doc in relevant_docs]\n",
    "    relevant_docs=relevant_docs[:num_reranker]\n",
    "    #Prompt\n",
    "    context='\\nExtracted documents:\\n'\n",
    "    context+=''.join([f'Document {str(i)}:::\\n'+doc for i, doc in enumerate(relevant_docs)])\n",
    "    prompt_final=prompt_template.format(question=question, context=context)\n",
    "    answer=generate(prompt_final)[0]['generated_text']\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can create a pipeline object using the following code snippet:\n",
      "\n",
      "```python\n",
      "from transformers import Pipeline\n",
      "\n",
      "# Define the pipeline parameters\n",
      "inputs = {\"prompt\": \"stained glass of darth vader, backlight, centered composition, masterpiece, photorealistic, 8k\"}\n",
      "\n",
      "# Create a new pipeline instance\n",
      "pipe = Pipeline(\n",
      "    # Specify the model name\n",
      "    model_name=\"stable-diffusion-v1-5\",\n",
      "    \n",
      "    # Specify the custom pipeline function\n",
      "    custom_pipeline=inputs[\"prompt\"],\n",
      "    \n",
      "    # Specify the output directory\n",
      "    output_dir=\"/path/to/output/directory\"\n",
      ")\n",
      "\n",
      "print(pipe)\n",
      "```\n",
      "\n",
      "This will create a pipeline object with the specified inputs and output directory.\n"
     ]
    }
   ],
   "source": [
    "question='How to create a pipeline object'\n",
    "answer=answer_rag(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chat demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_chat(text, sys_prompt='Welcome'):\n",
    "    answer=answer_rag(text)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo=gr.ChatInterface(\n",
    "    func_chat,\n",
    "    #type='messages',\n",
    "    textbox=gr.Textbox(placeholder='Enter question here', container=False, scale=7),\n",
    "    chatbot=gr.Chatbot(height=400),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
